[
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "soylemma",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soylemma",
        "description": "soylemma",
        "detail": "soylemma",
        "documentation": {}
    },
    {
        "label": "train_model_using_sejong_corpus_cleaner",
        "importPath": "soylemma",
        "description": "soylemma",
        "isExtraImport": true,
        "detail": "soylemma",
        "documentation": {}
    },
    {
        "label": "setuptools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "setuptools",
        "description": "setuptools",
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "LDAModel",
        "importPath": "ai_model.lda_model",
        "description": "ai_model.lda_model",
        "isExtraImport": true,
        "detail": "ai_model.lda_model",
        "documentation": {}
    },
    {
        "label": "LDAModel",
        "importPath": "ai_model.lda_model",
        "description": "ai_model.lda_model",
        "isExtraImport": true,
        "detail": "ai_model.lda_model",
        "documentation": {}
    },
    {
        "label": "LDAModel",
        "importPath": "ai_model.lda_model",
        "description": "ai_model.lda_model",
        "isExtraImport": true,
        "detail": "ai_model.lda_model",
        "documentation": {}
    },
    {
        "label": "LDAModel",
        "importPath": "ai_model.lda_model",
        "description": "ai_model.lda_model",
        "isExtraImport": true,
        "detail": "ai_model.lda_model",
        "documentation": {}
    },
    {
        "label": "LDAModel",
        "importPath": "ai_model.lda_model",
        "description": "ai_model.lda_model",
        "isExtraImport": true,
        "detail": "ai_model.lda_model",
        "documentation": {}
    },
    {
        "label": "RegressionModel",
        "importPath": "ai_model.regression_model",
        "description": "ai_model.regression_model",
        "isExtraImport": true,
        "detail": "ai_model.regression_model",
        "documentation": {}
    },
    {
        "label": "RegressionModel",
        "importPath": "ai_model.regression_model",
        "description": "ai_model.regression_model",
        "isExtraImport": true,
        "detail": "ai_model.regression_model",
        "documentation": {}
    },
    {
        "label": "RegressionModel",
        "importPath": "ai_model.regression_model",
        "description": "ai_model.regression_model",
        "isExtraImport": true,
        "detail": "ai_model.regression_model",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "corpora",
        "importPath": "gensim",
        "description": "gensim",
        "isExtraImport": true,
        "detail": "gensim",
        "documentation": {}
    },
    {
        "label": "LdaMulticore",
        "importPath": "gensim.models.ldamulticore",
        "description": "gensim.models.ldamulticore",
        "isExtraImport": true,
        "detail": "gensim.models.ldamulticore",
        "documentation": {}
    },
    {
        "label": "LdaModel",
        "importPath": "gensim.models",
        "description": "gensim.models",
        "isExtraImport": true,
        "detail": "gensim.models",
        "documentation": {}
    },
    {
        "label": "LdaModel",
        "importPath": "gensim.models",
        "description": "gensim.models",
        "isExtraImport": true,
        "detail": "gensim.models",
        "documentation": {}
    },
    {
        "label": "CoherenceModel",
        "importPath": "gensim.models.coherencemodel",
        "description": "gensim.models.coherencemodel",
        "isExtraImport": true,
        "detail": "gensim.models.coherencemodel",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "TextPreprocessor",
        "importPath": "ai_model.text_preprocessor",
        "description": "ai_model.text_preprocessor",
        "isExtraImport": true,
        "detail": "ai_model.text_preprocessor",
        "documentation": {}
    },
    {
        "label": "TextPreprocessor",
        "importPath": "ai_model.text_preprocessor",
        "description": "ai_model.text_preprocessor",
        "isExtraImport": true,
        "detail": "ai_model.text_preprocessor",
        "documentation": {}
    },
    {
        "label": "BaseConfig",
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "isExtraImport": true,
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "LDAModelConfig",
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "isExtraImport": true,
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "model_weights_path",
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "isExtraImport": true,
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "BaseConfig",
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "isExtraImport": true,
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "model_weights_path",
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "isExtraImport": true,
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "RegressionModelConfig",
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "isExtraImport": true,
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "VolaConfig",
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "isExtraImport": true,
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "dump",
        "importPath": "joblib",
        "description": "joblib",
        "isExtraImport": true,
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "load",
        "importPath": "joblib",
        "description": "joblib",
        "isExtraImport": true,
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "Ridge",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "Lasso",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "sklearn.pipeline",
        "description": "sklearn.pipeline",
        "isExtraImport": true,
        "detail": "sklearn.pipeline",
        "documentation": {}
    },
    {
        "label": "adjust_time",
        "importPath": "ai_model.utils",
        "description": "ai_model.utils",
        "isExtraImport": true,
        "detail": "ai_model.utils",
        "documentation": {}
    },
    {
        "label": "calculate_price_change",
        "importPath": "ai_model.utils",
        "description": "ai_model.utils",
        "isExtraImport": true,
        "detail": "ai_model.utils",
        "documentation": {}
    },
    {
        "label": "PeCab",
        "importPath": "pecab",
        "description": "pecab",
        "isExtraImport": true,
        "detail": "pecab",
        "documentation": {}
    },
    {
        "label": "Lemmatizer",
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "isExtraImport": true,
        "detail": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "documentation": {}
    },
    {
        "label": "pytimekr",
        "importPath": "pytimekr",
        "description": "pytimekr",
        "isExtraImport": true,
        "detail": "pytimekr",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "ModelService",
        "importPath": "service.model_service",
        "description": "service.model_service",
        "isExtraImport": true,
        "detail": "service.model_service",
        "documentation": {}
    },
    {
        "label": "Session",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "Session",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "Mapped",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "mapped_column",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "Mapped",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "mapped_column",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "Mapped",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "mapped_column",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "Session",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "Session",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "DeclarativeBase",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "sessionmaker",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "Base",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "Base",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "Base",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "create_db",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "create_db",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "create_db",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "NewsService",
        "importPath": "service.news_service",
        "description": "service.news_service",
        "isExtraImport": true,
        "detail": "service.news_service",
        "documentation": {}
    },
    {
        "label": "NewsService",
        "importPath": "service.news_service",
        "description": "service.news_service",
        "isExtraImport": true,
        "detail": "service.news_service",
        "documentation": {}
    },
    {
        "label": "NewsService",
        "importPath": "service.news_service",
        "description": "service.news_service",
        "isExtraImport": true,
        "detail": "service.news_service",
        "documentation": {}
    },
    {
        "label": "StockService",
        "importPath": "service.stock_service",
        "description": "service.stock_service",
        "isExtraImport": true,
        "detail": "service.stock_service",
        "documentation": {}
    },
    {
        "label": "DateTime",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Integer",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "String",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "DateTime",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Integer",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "String",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "DateTime",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Integer",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "select",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "select",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "select",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "select",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "select",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "inspect",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "News",
        "importPath": "model.news",
        "description": "model.news",
        "isExtraImport": true,
        "detail": "model.news",
        "documentation": {}
    },
    {
        "label": "News",
        "importPath": "model.news",
        "description": "model.news",
        "isExtraImport": true,
        "detail": "model.news",
        "documentation": {}
    },
    {
        "label": "News",
        "importPath": "model.news",
        "description": "model.news",
        "isExtraImport": true,
        "detail": "model.news",
        "documentation": {}
    },
    {
        "label": "News",
        "importPath": "model.news",
        "description": "model.news",
        "isExtraImport": true,
        "detail": "model.news",
        "documentation": {}
    },
    {
        "label": "BaseRepository",
        "importPath": "repository.base_repository",
        "description": "repository.base_repository",
        "isExtraImport": true,
        "detail": "repository.base_repository",
        "documentation": {}
    },
    {
        "label": "BaseRepository",
        "importPath": "repository.base_repository",
        "description": "repository.base_repository",
        "isExtraImport": true,
        "detail": "repository.base_repository",
        "documentation": {}
    },
    {
        "label": "BaseRepository",
        "importPath": "repository.base_repository",
        "description": "repository.base_repository",
        "isExtraImport": true,
        "detail": "repository.base_repository",
        "documentation": {}
    },
    {
        "label": "NewsPrediction",
        "importPath": "model.news_prediction",
        "description": "model.news_prediction",
        "isExtraImport": true,
        "detail": "model.news_prediction",
        "documentation": {}
    },
    {
        "label": "NewsPrediction",
        "importPath": "model.news_prediction",
        "description": "model.news_prediction",
        "isExtraImport": true,
        "detail": "model.news_prediction",
        "documentation": {}
    },
    {
        "label": "Stock",
        "importPath": "model.stock",
        "description": "model.stock",
        "isExtraImport": true,
        "detail": "model.stock",
        "documentation": {}
    },
    {
        "label": "Stock",
        "importPath": "model.stock",
        "description": "model.stock",
        "isExtraImport": true,
        "detail": "model.stock",
        "documentation": {}
    },
    {
        "label": "Stock",
        "importPath": "model.stock",
        "description": "model.stock",
        "isExtraImport": true,
        "detail": "model.stock",
        "documentation": {}
    },
    {
        "label": "Stock",
        "importPath": "model.stock",
        "description": "model.stock",
        "isExtraImport": true,
        "detail": "model.stock",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "DataController",
        "importPath": "ai_model.data_controller",
        "description": "ai_model.data_controller",
        "isExtraImport": true,
        "detail": "ai_model.data_controller",
        "documentation": {}
    },
    {
        "label": "DataController",
        "importPath": "ai_model.data_controller",
        "description": "ai_model.data_controller",
        "isExtraImport": true,
        "detail": "ai_model.data_controller",
        "documentation": {}
    },
    {
        "label": "DataController",
        "importPath": "ai_model.data_controller",
        "description": "ai_model.data_controller",
        "isExtraImport": true,
        "detail": "ai_model.data_controller",
        "documentation": {}
    },
    {
        "label": "NewsRepository",
        "importPath": "repository.news_repository",
        "description": "repository.news_repository",
        "isExtraImport": true,
        "detail": "repository.news_repository",
        "documentation": {}
    },
    {
        "label": "NewsRepository",
        "importPath": "repository.news_repository",
        "description": "repository.news_repository",
        "isExtraImport": true,
        "detail": "repository.news_repository",
        "documentation": {}
    },
    {
        "label": "NewsRepository",
        "importPath": "repository.news_repository",
        "description": "repository.news_repository",
        "isExtraImport": true,
        "detail": "repository.news_repository",
        "documentation": {}
    },
    {
        "label": "PredictionRepository",
        "importPath": "repository.prediction_repository",
        "description": "repository.prediction_repository",
        "isExtraImport": true,
        "detail": "repository.prediction_repository",
        "documentation": {}
    },
    {
        "label": "StockRepository",
        "importPath": "repository.stock_repository",
        "description": "repository.stock_repository",
        "isExtraImport": true,
        "detail": "repository.stock_repository",
        "documentation": {}
    },
    {
        "label": "StockRepository",
        "importPath": "repository.stock_repository",
        "description": "repository.stock_repository",
        "isExtraImport": true,
        "detail": "repository.stock_repository",
        "documentation": {}
    },
    {
        "label": "StockRepository",
        "importPath": "repository.stock_repository",
        "description": "repository.stock_repository",
        "isExtraImport": true,
        "detail": "repository.stock_repository",
        "documentation": {}
    },
    {
        "label": "BaseService",
        "importPath": "service.base_service",
        "description": "service.base_service",
        "isExtraImport": true,
        "detail": "service.base_service",
        "documentation": {}
    },
    {
        "label": "BaseService",
        "importPath": "service.base_service",
        "description": "service.base_service",
        "isExtraImport": true,
        "detail": "service.base_service",
        "documentation": {}
    },
    {
        "label": "BaseService",
        "importPath": "service.base_service",
        "description": "service.base_service",
        "isExtraImport": true,
        "detail": "service.base_service",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "webdriver",
        "importPath": "selenium",
        "description": "selenium",
        "isExtraImport": true,
        "detail": "selenium",
        "documentation": {}
    },
    {
        "label": "Options",
        "importPath": "selenium.webdriver.chrome.options",
        "description": "selenium.webdriver.chrome.options",
        "isExtraImport": true,
        "detail": "selenium.webdriver.chrome.options",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "isExtraImport": true,
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "win32com.client",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "win32com.client",
        "description": "win32com.client",
        "detail": "win32com.client",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "stock",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "uvicorn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uvicorn",
        "description": "uvicorn",
        "detail": "uvicorn",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "model_router",
        "importPath": "api.router.model",
        "description": "api.router.model",
        "isExtraImport": true,
        "detail": "api.router.model",
        "documentation": {}
    },
    {
        "label": "news_router",
        "importPath": "api.router.news",
        "description": "api.router.news",
        "isExtraImport": true,
        "detail": "api.router.news",
        "documentation": {}
    },
    {
        "label": "stock_router",
        "importPath": "api.router.stock",
        "description": "api.router.stock",
        "isExtraImport": true,
        "detail": "api.router.stock",
        "documentation": {}
    },
    {
        "label": "is_hangle",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "def is_hangle(word):\n    \"\"\"\n    Arguments\n    ---------\n    word : str\n    Returns\n    -------\n    It returns True if all characters in the word are Hangle\n    Else It returns False\n    \"\"\"",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "compose",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "def compose(cho, jung, jong):\n    \"\"\"\n    Arguments\n    ---------\n    cho : str\n        Chosung, length is 1\n    jung : str\n        Jungsung, length is 1\n    jong : str\n        Jongsung, length is 1",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "decompose",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "def decompose(input, ensure_input=False):\n    \"\"\"\n    Arguments\n    ---------\n    input : str\n        Character, length is 1\n    ensure_input : Boolean\n        If True, pass length and hangle check\n    Returns\n    -------",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "hangle_pattern",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "hangle_pattern = re.compile('[가-힣ㄱ-ㅎㅏ-ㅣ]+')\nis_jaum = lambda c: 'ㄱ' <= c <= 'ㅎ'\nis_moum = lambda c: 'ㅏ' <= c <= 'ㅣ'\ndef is_hangle(word):\n    \"\"\"\n    Arguments\n    ---------\n    word : str\n    Returns\n    -------",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "is_jaum",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "is_jaum = lambda c: 'ㄱ' <= c <= 'ㅎ'\nis_moum = lambda c: 'ㅏ' <= c <= 'ㅣ'\ndef is_hangle(word):\n    \"\"\"\n    Arguments\n    ---------\n    word : str\n    Returns\n    -------\n    It returns True if all characters in the word are Hangle",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "is_moum",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "is_moum = lambda c: 'ㅏ' <= c <= 'ㅣ'\ndef is_hangle(word):\n    \"\"\"\n    Arguments\n    ---------\n    word : str\n    Returns\n    -------\n    It returns True if all characters in the word are Hangle\n    Else It returns False",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "kor_begin",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "kor_begin = 44032\nkor_end = 55203\ncho_base = 588\njung_base = 28\njaum_begin = 12593\njaum_end = 12622\nmoum_begin = 12623\nmoum_end = 12643\n# 19 characters\nchosungs = [",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "kor_end",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "kor_end = 55203\ncho_base = 588\njung_base = 28\njaum_begin = 12593\njaum_end = 12622\nmoum_begin = 12623\nmoum_end = 12643\n# 19 characters\nchosungs = [\n    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ',",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "cho_base",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "cho_base = 588\njung_base = 28\njaum_begin = 12593\njaum_end = 12622\nmoum_begin = 12623\nmoum_end = 12643\n# 19 characters\nchosungs = [\n    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ',\n    'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "jung_base",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "jung_base = 28\njaum_begin = 12593\njaum_end = 12622\nmoum_begin = 12623\nmoum_end = 12643\n# 19 characters\nchosungs = [\n    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ',\n    'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',\n    'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ',",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "jaum_begin",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "jaum_begin = 12593\njaum_end = 12622\nmoum_begin = 12623\nmoum_end = 12643\n# 19 characters\nchosungs = [\n    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ',\n    'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',\n    'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ',\n    'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "jaum_end",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "jaum_end = 12622\nmoum_begin = 12623\nmoum_end = 12643\n# 19 characters\nchosungs = [\n    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ',\n    'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',\n    'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ',\n    'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n]",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "moum_begin",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "moum_begin = 12623\nmoum_end = 12643\n# 19 characters\nchosungs = [\n    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ',\n    'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',\n    'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ',\n    'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n]\n# 21 characters",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "moum_end",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "moum_end = 12643\n# 19 characters\nchosungs = [\n    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ',\n    'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',\n    'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ',\n    'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n]\n# 21 characters\njungsungs = [",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "chosungs",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "chosungs = [\n    'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ',\n    'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ',\n    'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ',\n    'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n]\n# 21 characters\njungsungs = [\n    'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ',\n    'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ',",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "jungsungs",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "jungsungs = [\n    'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ',\n    'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ',\n    'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ',\n    'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ',\n    'ㅣ'\n]\n# 28 characters including white space\njongsungs = [\n    ' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ',",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "jongsungs",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "jongsungs = [\n    ' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ',\n    'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ',\n    'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ',\n    'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ',\n    'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ',\n    'ㅌ', 'ㅍ', 'ㅎ'\n]\n# 30 characters\njaums = [",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "jaums",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "jaums = [\n    'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ',\n    'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ',\n    'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ',\n    'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅄ',\n    'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ',\n    'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ'\n]\n# 21 characters\nmoums = [",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "moums",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "moums = [\n    'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ',\n    'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ',\n    'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ',\n    'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ',\n    'ㅣ'\n]\ncho_to_idx = {cho:idx for idx, cho in enumerate(chosungs)}\njung_to_idx = {jung:idx for idx, jung in enumerate(jungsungs)}\njong_to_idx = {jong:idx for idx, jong in enumerate(jongsungs)}",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "cho_to_idx",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "cho_to_idx = {cho:idx for idx, cho in enumerate(chosungs)}\njung_to_idx = {jung:idx for idx, jung in enumerate(jungsungs)}\njong_to_idx = {jong:idx for idx, jong in enumerate(jongsungs)}",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "jung_to_idx",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "jung_to_idx = {jung:idx for idx, jung in enumerate(jungsungs)}\njong_to_idx = {jong:idx for idx, jong in enumerate(jongsungs)}",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "jong_to_idx",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "description": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "peekOfCode": "jong_to_idx = {jong:idx for idx, jong in enumerate(jongsungs)}",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.hangle",
        "documentation": {}
    },
    {
        "label": "Lemmatizer",
        "kind": 6,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "peekOfCode": "class Lemmatizer:\n    \"\"\"\n    Korean trained lemmatizer class\n    Arguments\n    ---------\n    verbs, adjectives, eomis : set of str\n        Dictionary set\n        If they are None, use trained dictionary.\n    lemma_rules : dict\n        Dictionary of lemmatization rules.",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "documentation": {}
    },
    {
        "label": "to_conjugate_rules",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "peekOfCode": "def to_conjugate_rules(lemma_rules):\n    # (하, 았) -> [했]\n    conjugate_rules = defaultdict(lambda: set())\n    for surf, canons in lemma_rules.items():\n        for stem, eomi in canons:\n            conjugate_rules[(stem, eomi)].add(surf)\n    return dict(conjugate_rules)\ndef analyze_morphology(word, verbs, adjectives, eomis, lemma_rules, debug=False):\n    \"\"\"\n    Arguments",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "documentation": {}
    },
    {
        "label": "analyze_morphology",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "peekOfCode": "def analyze_morphology(word, verbs, adjectives, eomis, lemma_rules, debug=False):\n    \"\"\"\n    Arguments\n    ---------\n    word : str\n        A word to analyze its morphology\n    verbs : set of str\n        Verb dictionary\n    adjectives : set of str\n        Adjective dictionary",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "documentation": {}
    },
    {
        "label": "get_lemma_candidates",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "peekOfCode": "def get_lemma_candidates(word, rules, debug=False):\n    \"\"\"\n    Arguments\n    ---------\n    word : str\n        A word to analyze its morphology\n    rules : dict of tuple\n        Lemmatization rules\n    Returns\n    -------",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "documentation": {}
    },
    {
        "label": "get_conjugate_candidates",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "peekOfCode": "def get_conjugate_candidates(stem, eomi, rules):\n    stem_ = stem[:-1]\n    eomi_ = eomi[1:]\n    key = (stem[-1], eomi[0])\n    candidates = ['{}{}{}'.format(stem_, surface, eomi_) for surface in rules.get(key, {})]\n    if len(eomi) >= 2:\n        key = (stem[-1], eomi[:2])\n        eomi_ = eomi[2:]\n        candidates += ['{}{}{}'.format(stem_, surface, eomi_) for surface in rules.get(key, {})]\n    candidates.append(stem + eomi)",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "documentation": {}
    },
    {
        "label": "check_rules",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "peekOfCode": "def check_rules(rules):\n    def type_error():\n        raise ValueError(\"Wrong format inserted rules. rules={surface:{(stem, eomi), (stem, eomi), ...}}\")\n    rules_ = {}\n    try:\n        for surface, canons in rules.items():\n            if isinstance(canons, str) or not isinstance(surface, str):\n                type_error()\n            canons_ = set()\n            for canon in canons:",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "documentation": {}
    },
    {
        "label": "update_rules",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "peekOfCode": "def update_rules(base, supplement):\n    for surface, supple_set in supplement.items():\n        base_set = base.get(surface, set())\n        base_set.update(supple_set)\n        base[surface] = base_set\n    return base",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.lemmatizer",
        "documentation": {}
    },
    {
        "label": "is_right_root",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "def is_right_root(morpheme):\n    return not root_pattern.findall(morpheme)\ndef is_right_eomi(morpheme):\n    return not eomi_pattern.findall(morpheme)\ndef parse(line):\n    eojeol, morphtags, count = line.strip().split('\\t')\n    morphtags = [mt.rsplit('/', 1) for mt in morphtags.split(' + ')]\n    count = int(count)\n    return eojeol, morphtags, count\ndef _right_form(morph):",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "is_right_eomi",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "def is_right_eomi(morpheme):\n    return not eomi_pattern.findall(morpheme)\ndef parse(line):\n    eojeol, morphtags, count = line.strip().split('\\t')\n    morphtags = [mt.rsplit('/', 1) for mt in morphtags.split(' + ')]\n    count = int(count)\n    return eojeol, morphtags, count\ndef _right_form(morph):\n    \"\"\"\n    Arguments",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "def parse(line):\n    eojeol, morphtags, count = line.strip().split('\\t')\n    morphtags = [mt.rsplit('/', 1) for mt in morphtags.split(' + ')]\n    count = int(count)\n    return eojeol, morphtags, count\ndef _right_form(morph):\n    \"\"\"\n    Arguments\n    ---------\n    morph : str",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "right_form",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "def right_form(morphemes):\n    for morph, _ in morphemes:\n        if not _right_form(morph):\n            return False\n    return True\ndef load_word_morpheme_table(path):\n    \"\"\"\n    Arguments\n    ---------\n    path : str",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "load_word_morpheme_table",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "def load_word_morpheme_table(path):\n    \"\"\"\n    Arguments\n    ---------\n    path : str\n        Eojeol, Morpheme, Count table\n        File example, \n            개봉된\t개봉되/Verb + ㄴ/Eomi\t17\n            개봉될\t개봉되/Verb + ㄹ/Eomi\t7\n            개봉인\t개봉이/Adjective + ㄴ/Eomi\t2",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "extract_rule",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "def extract_rule(eojeol, lw, lt, rw, rt):\n    \"\"\"\n    Arguments\n    ---------\n    eojeol : str\n        Eojeol\n    lr : str\n        Left-side morpheme\n    lt : str\n        Tag of left-side morpheme",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "extract_rules",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "def extract_rules(eojeol_lr_array):\n    \"\"\"\n    Arguments\n    ---------\n    eojeol_lr_array : nested list\n        [\n            (Eojeol, ((lw, lt), (rw, rt))),\n            (Eojeol, ((lw, lt), (rw, rt))),\n            ...\n        ]",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "train_model_using_sejong_corpus_cleaner",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "def train_model_using_sejong_corpus_cleaner(local_repository_path, table_path, show_exception=False):\n    \"\"\"\n    Arguments\n    ---------\n    local_repository_path : str\n        Local repository path of https://github.com/loit/sejong_corpus_cleaner.git\n    table_path : str\n        Count table path\n        A row in the table is formed such as ((Eojeol, MorphTags), count)\n    show_exception : Boolean",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "is_jaum",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "is_jaum = lambda c: 'ㄱ' <= c <= 'ㅎ'\nis_moum = lambda c: 'ㅏ' <= c <= 'ㅣ'\nroot_pattern = re.compile('[^가-힣]+')\neomi_pattern = re.compile('[^가-힣ㄱ-ㅎㅏ-ㅣ]')\ndef is_right_root(morpheme):\n    return not root_pattern.findall(morpheme)\ndef is_right_eomi(morpheme):\n    return not eomi_pattern.findall(morpheme)\ndef parse(line):\n    eojeol, morphtags, count = line.strip().split('\\t')",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "is_moum",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "is_moum = lambda c: 'ㅏ' <= c <= 'ㅣ'\nroot_pattern = re.compile('[^가-힣]+')\neomi_pattern = re.compile('[^가-힣ㄱ-ㅎㅏ-ㅣ]')\ndef is_right_root(morpheme):\n    return not root_pattern.findall(morpheme)\ndef is_right_eomi(morpheme):\n    return not eomi_pattern.findall(morpheme)\ndef parse(line):\n    eojeol, morphtags, count = line.strip().split('\\t')\n    morphtags = [mt.rsplit('/', 1) for mt in morphtags.split(' + ')]",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "root_pattern",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "root_pattern = re.compile('[^가-힣]+')\neomi_pattern = re.compile('[^가-힣ㄱ-ㅎㅏ-ㅣ]')\ndef is_right_root(morpheme):\n    return not root_pattern.findall(morpheme)\ndef is_right_eomi(morpheme):\n    return not eomi_pattern.findall(morpheme)\ndef parse(line):\n    eojeol, morphtags, count = line.strip().split('\\t')\n    morphtags = [mt.rsplit('/', 1) for mt in morphtags.split(' + ')]\n    count = int(count)",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "eomi_pattern",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "description": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "peekOfCode": "eomi_pattern = re.compile('[^가-힣ㄱ-ㅎㅏ-ㅣ]')\ndef is_right_root(morpheme):\n    return not root_pattern.findall(morpheme)\ndef is_right_eomi(morpheme):\n    return not eomi_pattern.findall(morpheme)\ndef parse(line):\n    eojeol, morphtags, count = line.strip().split('\\t')\n    morphtags = [mt.rsplit('/', 1) for mt in morphtags.split(' + ')]\n    count = int(count)\n    return eojeol, morphtags, count",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.trainer",
        "documentation": {}
    },
    {
        "label": "installpath",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "description": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "peekOfCode": "installpath = os.path.sep.join(\n    os.path.dirname(os.path.realpath(__file__)).split(os.path.sep)[:-1])\nADJECTIVE = 'Adjective'\nVERB = 'Verb'\nEOMI = 'Eomi'",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "documentation": {}
    },
    {
        "label": "ADJECTIVE",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "description": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "peekOfCode": "ADJECTIVE = 'Adjective'\nVERB = 'Verb'\nEOMI = 'Eomi'",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "documentation": {}
    },
    {
        "label": "VERB",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "description": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "peekOfCode": "VERB = 'Verb'\nEOMI = 'Eomi'",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "documentation": {}
    },
    {
        "label": "EOMI",
        "kind": 5,
        "importPath": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "description": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "peekOfCode": "EOMI = 'Eomi'",
        "detail": "ai_model.korean_lemmatizer_master.soylemma.utils",
        "documentation": {}
    },
    {
        "label": "prune_dictionary",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.update_model",
        "description": "ai_model.korean_lemmatizer_master.update_model",
        "peekOfCode": "def prune_dictionary(dic, min_count):\n    return {w:c for w,c in dic.items() if c >= min_count}\ndef save_dictionary(dic, path):\n    with open(path, 'w', encoding='utf-8') as f:\n        for morpheme, count in sorted(dic.items()):\n            f.write('{} {}\\n'.format(morpheme, count))\ndef save_rules(rules, path):\n    with open(path, 'w', encoding='utf-8') as f:\n        for surface, canons in sorted(rules.items()):\n            for l, r in sorted(canons):",
        "detail": "ai_model.korean_lemmatizer_master.update_model",
        "documentation": {}
    },
    {
        "label": "save_dictionary",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.update_model",
        "description": "ai_model.korean_lemmatizer_master.update_model",
        "peekOfCode": "def save_dictionary(dic, path):\n    with open(path, 'w', encoding='utf-8') as f:\n        for morpheme, count in sorted(dic.items()):\n            f.write('{} {}\\n'.format(morpheme, count))\ndef save_rules(rules, path):\n    with open(path, 'w', encoding='utf-8') as f:\n        for surface, canons in sorted(rules.items()):\n            for l, r in sorted(canons):\n                f.write('{} {} {}\\n'.format(surface, l, r))\ndef save_exceptions(exceptions):",
        "detail": "ai_model.korean_lemmatizer_master.update_model",
        "documentation": {}
    },
    {
        "label": "save_rules",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.update_model",
        "description": "ai_model.korean_lemmatizer_master.update_model",
        "peekOfCode": "def save_rules(rules, path):\n    with open(path, 'w', encoding='utf-8') as f:\n        for surface, canons in sorted(rules.items()):\n            for l, r in sorted(canons):\n                f.write('{} {} {}\\n'.format(surface, l, r))\ndef save_exceptions(exceptions):\n    with open('exception_cases_logs', 'w', encoding='utf-8') as f:\n        for exception, count in sorted(exceptions.items(), key=lambda x:-x[1]):\n            exception_strf = ', '.join(exception)\n            f.write('{}\\t{}\\n'.format(exception_strf, count))",
        "detail": "ai_model.korean_lemmatizer_master.update_model",
        "documentation": {}
    },
    {
        "label": "save_exceptions",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.update_model",
        "description": "ai_model.korean_lemmatizer_master.update_model",
        "peekOfCode": "def save_exceptions(exceptions):\n    with open('exception_cases_logs', 'w', encoding='utf-8') as f:\n        for exception, count in sorted(exceptions.items(), key=lambda x:-x[1]):\n            exception_strf = ', '.join(exception)\n            f.write('{}\\t{}\\n'.format(exception_strf, count))\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--sejong_corpus_cleaner_repository', type=str, default='../sejong_corpus_cleaner/',\n        help='Local repository path of https://github.com/lovit/sejong_corpus_cleaner/')\n    parser.add_argument('--corpus_type', type=str, default='type3', choices=['type1', 'type2', 'type3'],",
        "detail": "ai_model.korean_lemmatizer_master.update_model",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ai_model.korean_lemmatizer_master.update_model",
        "description": "ai_model.korean_lemmatizer_master.update_model",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--sejong_corpus_cleaner_repository', type=str, default='../sejong_corpus_cleaner/',\n        help='Local repository path of https://github.com/lovit/sejong_corpus_cleaner/')\n    parser.add_argument('--corpus_type', type=str, default='type3', choices=['type1', 'type2', 'type3'],\n        help='L-R corpus type')\n    parser.add_argument('--min_count', type=int, default=1, help='Minimum frequency of morphemes in dictionary')\n    parser.add_argument('--dictionary_name', type=str, default='default', help='Dictioanry name')\n    args = parser.parse_args()\n    local_repository_path = args.sejong_corpus_cleaner_repository",
        "detail": "ai_model.korean_lemmatizer_master.update_model",
        "documentation": {}
    },
    {
        "label": "LDAModelConfig",
        "kind": 6,
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "peekOfCode": "class LDAModelConfig:\n    NUM_OF_CATEGORY = range(20, 101)\n    NUM_OF_TOPICS_BY_GROUP = range(5, 21)\n    PASSES = 100\nclass RegressionModelConfig:\n    RIDGE_PARAMETERS = {\"ridge__alpha\": [1e-4, 1e-2, 1, 10, 100], \"ridge__random_state\": [25]}\n    LASSO_PARAMETERS = {\"lasso__alpha\": [1e-4, 1e-2, 1, 10, 100], \"lasso__random_state\": [25]}\nclass BaseConfig:\n    TEST_SIZE = 0.3\n    RANDOM_STATE = 25",
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "RegressionModelConfig",
        "kind": 6,
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "peekOfCode": "class RegressionModelConfig:\n    RIDGE_PARAMETERS = {\"ridge__alpha\": [1e-4, 1e-2, 1, 10, 100], \"ridge__random_state\": [25]}\n    LASSO_PARAMETERS = {\"lasso__alpha\": [1e-4, 1e-2, 1, 10, 100], \"lasso__random_state\": [25]}\nclass BaseConfig:\n    TEST_SIZE = 0.3\n    RANDOM_STATE = 25\nclass VolaConfig:\n    TIME_INTERVALS = [1, 5, 15, 60, 1440]\n    VOLA_COLUMNS = [f\"vola_{time}m\" for time in TIME_INTERVALS]",
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "BaseConfig",
        "kind": 6,
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "peekOfCode": "class BaseConfig:\n    TEST_SIZE = 0.3\n    RANDOM_STATE = 25\nclass VolaConfig:\n    TIME_INTERVALS = [1, 5, 15, 60, 1440]\n    VOLA_COLUMNS = [f\"vola_{time}m\" for time in TIME_INTERVALS]",
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "VolaConfig",
        "kind": 6,
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "peekOfCode": "class VolaConfig:\n    TIME_INTERVALS = [1, 5, 15, 60, 1440]\n    VOLA_COLUMNS = [f\"vola_{time}m\" for time in TIME_INTERVALS]",
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "model_weights_path",
        "kind": 5,
        "importPath": "ai_model.constants",
        "description": "ai_model.constants",
        "peekOfCode": "model_weights_path = \"/home/tako4/capstone/backend/Model/Backend/ai_model/model_weights\"\nclass LDAModelConfig:\n    NUM_OF_CATEGORY = range(20, 101)\n    NUM_OF_TOPICS_BY_GROUP = range(5, 21)\n    PASSES = 100\nclass RegressionModelConfig:\n    RIDGE_PARAMETERS = {\"ridge__alpha\": [1e-4, 1e-2, 1, 10, 100], \"ridge__random_state\": [25]}\n    LASSO_PARAMETERS = {\"lasso__alpha\": [1e-4, 1e-2, 1, 10, 100], \"lasso__random_state\": [25]}\nclass BaseConfig:\n    TEST_SIZE = 0.3",
        "detail": "ai_model.constants",
        "documentation": {}
    },
    {
        "label": "DataController",
        "kind": 6,
        "importPath": "ai_model.data_controller",
        "description": "ai_model.data_controller",
        "peekOfCode": "class DataController:\n    def __init__(self):\n        pass\n    def train_news_dataset(self, news_dataset: pd.DataFrame, stock_dataset: pd.DataFrame):\n        \"\"\"\n        주기적으로 새로운 뉴스 데이터 + 기존 데이터 세트에 대해서 LDA 재추출 및 회귀 분석 실시 API\n        Args:\n            news_dataset: 뉴스 데이터 세트 [필요한 컬럼 - date_time, content(documents)]\n            stock_dataset: 종목 1분봉 데이터 세트 [필요한 컬럼 - date_time, price]\n        \"\"\"",
        "detail": "ai_model.data_controller",
        "documentation": {}
    },
    {
        "label": "LDAModel",
        "kind": 6,
        "importPath": "ai_model.lda_model",
        "description": "ai_model.lda_model",
        "peekOfCode": "class LDAModel:\n    def __init__(self):\n        self.df = pd.DataFrame()\n        self.grouped_dfs = list()\n    # For Training\n    ###########################################################################################\n    def train_lda_model(self, dataset: pd.DataFrame):\n        \"\"\"\n        주기적으로 LDA모델들을 학습시키기 위한 함수\n        뉴스 데이터 세트를 통해 하이퍼파라미터 튜닝부터 폴더링, topic 별 lda 모델 생성, 가중치 저장까지의 프로세스 자동화",
        "detail": "ai_model.lda_model",
        "documentation": {}
    },
    {
        "label": "RegressionModel",
        "kind": 6,
        "importPath": "ai_model.regression_model",
        "description": "ai_model.regression_model",
        "peekOfCode": "class RegressionModel:\n    def __init__(self, stock_dataset: pd.DataFrame = None, lda_model: LDAModel = None):\n        \"\"\"\n        Args:\n            stock_dataset: 종목의 1분봉 주가 데이터\n        \"\"\"\n        self.stock_dataset = stock_dataset\n        self.grouped_dfs = lda_model.get_group_df() if lda_model else None\n    def train_regression_model(self, num_topics):\n        \"\"\"",
        "detail": "ai_model.regression_model",
        "documentation": {}
    },
    {
        "label": "TextPreprocessor",
        "kind": 6,
        "importPath": "ai_model.text_preprocessor",
        "description": "ai_model.text_preprocessor",
        "peekOfCode": "class TextPreprocessor:\n    def __init__(self, texts: List[str] or str):\n        self.texts = texts\n    def preprocess(self):\n        self._analyze_morphs()\n        self._remove_stopwords()\n        self._remove_special_characters()\n        self._lemmatize_filtered_morphs()\n        self._join_texts()\n        return self.texts",
        "detail": "ai_model.text_preprocessor",
        "documentation": {}
    },
    {
        "label": "adjust_time",
        "kind": 2,
        "importPath": "ai_model.utils",
        "description": "ai_model.utils",
        "peekOfCode": "def adjust_time(time):\n    \"\"\"\n    다음 거래일 09:00 설정을 위한 함수\n    \"\"\"\n    def next_business_day_9am(current_day):\n        next_day = current_day.date() + timedelta(days=1)\n        # 다음 날이 공휴일이거나 주말인 경우 다음 가능한 평일로 이동\n        while next_day in pytimekr.holidays(next_day.year) or next_day.weekday() >= 5:\n            next_day += timedelta(days=1)\n        return datetime.combine(next_day, datetime.strptime(\"09:00\", \"%H:%M\").time())",
        "detail": "ai_model.utils",
        "documentation": {}
    },
    {
        "label": "calculate_price_change",
        "kind": 2,
        "importPath": "ai_model.utils",
        "description": "ai_model.utils",
        "peekOfCode": "def calculate_price_change(current_time, minutes, stock_dataset: pd.DataFrame):\n    \"\"\"\n    주가 변화량 계산 함수\n    Args:\n        current_time: DataFrame 객체의 date_time 열 값\n        minutes: 특정 분 후의 시간\n        stock_dataset: 종목의 1분봉 주가 데이터 세트\n    Returns:\n        주가 변화량 계산 값\n    \"\"\"",
        "detail": "ai_model.utils",
        "documentation": {}
    },
    {
        "label": "get_prediction_by_news_id",
        "kind": 2,
        "importPath": "api.router.model",
        "description": "api.router.model",
        "peekOfCode": "def get_prediction_by_news_id(news_id: str):\n    return ModelService().get_prediction(news_id=news_id)",
        "detail": "api.router.model",
        "documentation": {}
    },
    {
        "label": "model_router",
        "kind": 5,
        "importPath": "api.router.model",
        "description": "api.router.model",
        "peekOfCode": "model_router = APIRouter()\n@model_router.get(\"/prediction/{news_id}\")\ndef get_prediction_by_news_id(news_id: str):\n    return ModelService().get_prediction(news_id=news_id)",
        "detail": "api.router.model",
        "documentation": {}
    },
    {
        "label": "get_news_by_news_id",
        "kind": 2,
        "importPath": "api.router.news",
        "description": "api.router.news",
        "peekOfCode": "def get_news_by_news_id(news_id: str, session: Session = Depends(get_session)):\n    res = NewsService(session=session).get_news_data(news_id=news_id)\n    return res\n@news_router.get(\"/list/{item_name}/{page}\")\ndef get_news_list_by_name(item_name: str, page: int, session: Session = Depends(get_session)):\n    return NewsService(session=session).get_news_list(item_name=item_name, page=page)",
        "detail": "api.router.news",
        "documentation": {}
    },
    {
        "label": "get_news_list_by_name",
        "kind": 2,
        "importPath": "api.router.news",
        "description": "api.router.news",
        "peekOfCode": "def get_news_list_by_name(item_name: str, page: int, session: Session = Depends(get_session)):\n    return NewsService(session=session).get_news_list(item_name=item_name, page=page)",
        "detail": "api.router.news",
        "documentation": {}
    },
    {
        "label": "news_router",
        "kind": 5,
        "importPath": "api.router.news",
        "description": "api.router.news",
        "peekOfCode": "news_router = APIRouter()\n@news_router.get(\"/{news_id}\")\ndef get_news_by_news_id(news_id: str, session: Session = Depends(get_session)):\n    res = NewsService(session=session).get_news_data(news_id=news_id)\n    return res\n@news_router.get(\"/list/{item_name}/{page}\")\ndef get_news_list_by_name(item_name: str, page: int, session: Session = Depends(get_session)):\n    return NewsService(session=session).get_news_list(item_name=item_name, page=page)",
        "detail": "api.router.news",
        "documentation": {}
    },
    {
        "label": "save_stock_data",
        "kind": 2,
        "importPath": "api.router.stock",
        "description": "api.router.stock",
        "peekOfCode": "def save_stock_data(time: int, price: int, session: Session = Depends(get_session)):\n    res = StockService(session=session).save_stock_data(time=time, price=price)\n    session.commit()\n    return res",
        "detail": "api.router.stock",
        "documentation": {}
    },
    {
        "label": "stock_router",
        "kind": 5,
        "importPath": "api.router.stock",
        "description": "api.router.stock",
        "peekOfCode": "stock_router = APIRouter()\n@stock_router.post(\"/save/{time}/{price}\")\ndef save_stock_data(time: int, price: int, session: Session = Depends(get_session)):\n    res = StockService(session=session).save_stock_data(time=time, price=price)\n    session.commit()\n    return res",
        "detail": "api.router.stock",
        "documentation": {}
    },
    {
        "label": "News",
        "kind": 6,
        "importPath": "model.news",
        "description": "model.news",
        "peekOfCode": "class News(Base):\n    __tablename__ = \"news\"\n    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)\n    news_url: Mapped[int] = mapped_column(String, nullable=True)\n    date_time: Mapped[datetime] = mapped_column(DateTime(timezone=True), index=True)\n    title: Mapped[str] = mapped_column(String, nullable=True)\n    writer: Mapped[str] = mapped_column(String, nullable=True)\n    content: Mapped[str] = mapped_column(String)",
        "detail": "model.news",
        "documentation": {}
    },
    {
        "label": "NewsPrediction",
        "kind": 6,
        "importPath": "model.news_prediction",
        "description": "model.news_prediction",
        "peekOfCode": "class NewsPrediction(Base):\n    __tablename__ = \"news_prediction\"\n    id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True)\n    news_id: Mapped[str] = mapped_column(String)\n    time: Mapped[datetime] = mapped_column(DateTime(timezone=True), index=True)\n    min_1: Mapped[int] = mapped_column(Integer)\n    min_5: Mapped[int] = mapped_column(Integer)\n    min_15: Mapped[int] = mapped_column(Integer)\n    min_60: Mapped[int] = mapped_column(Integer)\n    day_1: Mapped[int] = mapped_column(Integer)",
        "detail": "model.news_prediction",
        "documentation": {}
    },
    {
        "label": "Stock",
        "kind": 6,
        "importPath": "model.stock",
        "description": "model.stock",
        "peekOfCode": "class Stock(Base):\n    __tablename__ = \"samsung\"\n    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)\n    date_time: Mapped[datetime] = mapped_column(DateTime(timezone=True), index=True)\n    price: Mapped[int] = mapped_column(Integer, nullable=False)",
        "detail": "model.stock",
        "documentation": {}
    },
    {
        "label": "BaseRepository",
        "kind": 6,
        "importPath": "repository.base_repository",
        "description": "repository.base_repository",
        "peekOfCode": "class BaseRepository:\n    def __init__(self, session: Session) -> None:\n        self.session: Session = session",
        "detail": "repository.base_repository",
        "documentation": {}
    },
    {
        "label": "NewsRepository",
        "kind": 6,
        "importPath": "repository.news_repository",
        "description": "repository.news_repository",
        "peekOfCode": "class NewsRepository(BaseRepository):\n    def save_news_data(self, news: News):\n        self.session.add(news)\n        return news\n    def get_news_data(self, news_url: str):\n        stmt = select(News).where(News.news_url == news_url)\n        res = self.session.execute(stmt)\n        res = res.scalar()\n        return res\n    def get_news_dataset(self) -> pd.DataFrame:",
        "detail": "repository.news_repository",
        "documentation": {}
    },
    {
        "label": "PredictionRepository",
        "kind": 6,
        "importPath": "repository.prediction_repository",
        "description": "repository.prediction_repository",
        "peekOfCode": "class PredictionRepository(BaseRepository):\n    def get_news_prediction(self, news_id: int):\n        stmt = select(NewsPrediction).where(NewsPrediction.news_id == news_id)\n        res = self.session.execute(stmt)\n        res = res.scalar()\n        return res\n    def save_news_prediction(self, news_prediction: NewsPrediction):\n        self.session.add(news_prediction)\n        return news_prediction",
        "detail": "repository.prediction_repository",
        "documentation": {}
    },
    {
        "label": "StockRepository",
        "kind": 6,
        "importPath": "repository.stock_repository",
        "description": "repository.stock_repository",
        "peekOfCode": "class StockRepository(BaseRepository):\n    def get_stock_data_by_date(self, date_time: datetime) -> int:\n        stmt = select(Stock).where(Stock.date_time == date_time)\n        res = self.session.execute(stmt)\n        return res.scalar().price\n    def save_stock_data(self, time: int, price: int):\n        stock_data = Stock()\n        date = datetime.datetime.today().date().strftime(\"%Y%m%d\")\n        time = str(time)\n        stock_data.date_time = datetime.datetime.strptime(date + time, \"%Y%m%d%H%M\")",
        "detail": "repository.stock_repository",
        "documentation": {}
    },
    {
        "label": "CreateNewsPrediction",
        "kind": 6,
        "importPath": "schema.news_prediction",
        "description": "schema.news_prediction",
        "peekOfCode": "class CreateNewsPrediction(BaseModel):\n    news_id: str\n    time: str\nclass NewsPrediction(CreateNewsPrediction):\n    min_1: int\n    min_5: int\n    min_15: int\n    min_60: int\n    day_1: int\nclass NewsPredictionResponse(NewsPrediction):",
        "detail": "schema.news_prediction",
        "documentation": {}
    },
    {
        "label": "NewsPrediction",
        "kind": 6,
        "importPath": "schema.news_prediction",
        "description": "schema.news_prediction",
        "peekOfCode": "class NewsPrediction(CreateNewsPrediction):\n    min_1: int\n    min_5: int\n    min_15: int\n    min_60: int\n    day_1: int\nclass NewsPredictionResponse(NewsPrediction):\n    pass",
        "detail": "schema.news_prediction",
        "documentation": {}
    },
    {
        "label": "NewsPredictionResponse",
        "kind": 6,
        "importPath": "schema.news_prediction",
        "description": "schema.news_prediction",
        "peekOfCode": "class NewsPredictionResponse(NewsPrediction):\n    pass",
        "detail": "schema.news_prediction",
        "documentation": {}
    },
    {
        "label": "BaseService",
        "kind": 6,
        "importPath": "service.base_service",
        "description": "service.base_service",
        "peekOfCode": "class BaseService:\n    def __init__(self, session: Session) -> None:\n        self.session = session",
        "detail": "service.base_service",
        "documentation": {}
    },
    {
        "label": "clean_reporter",
        "kind": 2,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "def clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)\n    cleantext = CLEANP2.sub(\"\", cleantext)\n    cleantext = CLEANP3.sub(\"\", cleantext)\n    cleantext = CLEANYTN.sub(\"\", cleantext)\n    return cleantext\ndef clean_tag(text):\n    cleantext = CLEANTAG.sub(\"\", text)\n    cleantext = CLEANSB.sub(\"\", cleantext)\n    return cleantext",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "clean_tag",
        "kind": 2,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "def clean_tag(text):\n    cleantext = CLEANTAG.sub(\"\", text)\n    cleantext = CLEANSB.sub(\"\", cleantext)\n    return cleantext\ndef clean_others(text):\n    cleantext = CLEANTH.sub(\"\", text)\n    cleantext = CLEANDT.sub(\"\", cleantext)\n    cleantext = re.sub(\"\\n\", \" \", cleantext)\n    cleantext = re.sub(\"\\s{2,}\", \" \", cleantext)\n    return cleantext",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "clean_others",
        "kind": 2,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "def clean_others(text):\n    cleantext = CLEANTH.sub(\"\", text)\n    cleantext = CLEANDT.sub(\"\", cleantext)\n    cleantext = re.sub(\"\\n\", \" \", cleantext)\n    cleantext = re.sub(\"\\s{2,}\", \" \", cleantext)\n    return cleantext\ndef clean_text(text):\n    cleantext = clean_reporter(text)\n    cleantext = clean_tag(cleantext)\n    cleantext = clean_others(cleantext)",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "kind": 2,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "def clean_text(text):\n    cleantext = clean_reporter(text)\n    cleantext = clean_tag(cleantext)\n    cleantext = clean_others(cleantext)\n    return cleantext",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANTAG",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANTAG = re.compile(\"<.*?>\")  # HTML tag\nCLEANSB = re.compile(\"\\[.*?\\]\")  # [xxx]\nCLEANP = re.compile(\"\\(.*?=.*?\\)\")  # (xxx = xxx)\nCLEANP2 = re.compile(\".*?=.*?\")  # xxx = <content>\nCLEANP3 = re.compile(\"(/[가-힣]*.\\w?기자)|(/뉴스.*?)\")\nCLEANYTN = re.compile(\"YTN 검색해 채널 추가\")\nCLEANSP = re.compile(\"[#\\?^@*\\\"※▶■◆▲●Δ▷◇☞~ㆍ!』‘|`'…》\\”\\“\\’]\")\nCLEANDT = re.compile(r\"\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}\")\nCLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANSB",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANSB = re.compile(\"\\[.*?\\]\")  # [xxx]\nCLEANP = re.compile(\"\\(.*?=.*?\\)\")  # (xxx = xxx)\nCLEANP2 = re.compile(\".*?=.*?\")  # xxx = <content>\nCLEANP3 = re.compile(\"(/[가-힣]*.\\w?기자)|(/뉴스.*?)\")\nCLEANYTN = re.compile(\"YTN 검색해 채널 추가\")\nCLEANSP = re.compile(\"[#\\?^@*\\\"※▶■◆▲●Δ▷◇☞~ㆍ!』‘|`'…》\\”\\“\\’]\")\nCLEANDT = re.compile(r\"\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}\")\nCLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANP",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANP = re.compile(\"\\(.*?=.*?\\)\")  # (xxx = xxx)\nCLEANP2 = re.compile(\".*?=.*?\")  # xxx = <content>\nCLEANP3 = re.compile(\"(/[가-힣]*.\\w?기자)|(/뉴스.*?)\")\nCLEANYTN = re.compile(\"YTN 검색해 채널 추가\")\nCLEANSP = re.compile(\"[#\\?^@*\\\"※▶■◆▲●Δ▷◇☞~ㆍ!』‘|`'…》\\”\\“\\’]\")\nCLEANDT = re.compile(r\"\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}\")\nCLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)\n    cleantext = CLEANP2.sub(\"\", cleantext)",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANP2",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANP2 = re.compile(\".*?=.*?\")  # xxx = <content>\nCLEANP3 = re.compile(\"(/[가-힣]*.\\w?기자)|(/뉴스.*?)\")\nCLEANYTN = re.compile(\"YTN 검색해 채널 추가\")\nCLEANSP = re.compile(\"[#\\?^@*\\\"※▶■◆▲●Δ▷◇☞~ㆍ!』‘|`'…》\\”\\“\\’]\")\nCLEANDT = re.compile(r\"\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}\")\nCLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)\n    cleantext = CLEANP2.sub(\"\", cleantext)\n    cleantext = CLEANP3.sub(\"\", cleantext)",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANP3",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANP3 = re.compile(\"(/[가-힣]*.\\w?기자)|(/뉴스.*?)\")\nCLEANYTN = re.compile(\"YTN 검색해 채널 추가\")\nCLEANSP = re.compile(\"[#\\?^@*\\\"※▶■◆▲●Δ▷◇☞~ㆍ!』‘|`'…》\\”\\“\\’]\")\nCLEANDT = re.compile(r\"\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}\")\nCLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)\n    cleantext = CLEANP2.sub(\"\", cleantext)\n    cleantext = CLEANP3.sub(\"\", cleantext)\n    cleantext = CLEANYTN.sub(\"\", cleantext)",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANYTN",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANYTN = re.compile(\"YTN 검색해 채널 추가\")\nCLEANSP = re.compile(\"[#\\?^@*\\\"※▶■◆▲●Δ▷◇☞~ㆍ!』‘|`'…》\\”\\“\\’]\")\nCLEANDT = re.compile(r\"\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}\")\nCLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)\n    cleantext = CLEANP2.sub(\"\", cleantext)\n    cleantext = CLEANP3.sub(\"\", cleantext)\n    cleantext = CLEANYTN.sub(\"\", cleantext)\n    return cleantext",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANSP",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANSP = re.compile(\"[#\\?^@*\\\"※▶■◆▲●Δ▷◇☞~ㆍ!』‘|`'…》\\”\\“\\’]\")\nCLEANDT = re.compile(r\"\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}\")\nCLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)\n    cleantext = CLEANP2.sub(\"\", cleantext)\n    cleantext = CLEANP3.sub(\"\", cleantext)\n    cleantext = CLEANYTN.sub(\"\", cleantext)\n    return cleantext\ndef clean_tag(text):",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANDT",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANDT = re.compile(r\"\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}\")\nCLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)\n    cleantext = CLEANP2.sub(\"\", cleantext)\n    cleantext = CLEANP3.sub(\"\", cleantext)\n    cleantext = CLEANYTN.sub(\"\", cleantext)\n    return cleantext\ndef clean_tag(text):\n    cleantext = CLEANTAG.sub(\"\", text)",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "CLEANTH",
        "kind": 5,
        "importPath": "service.cleaner",
        "description": "service.cleaner",
        "peekOfCode": "CLEANTH = re.compile(\"\\d$\")\ndef clean_reporter(text):\n    cleantext = CLEANP.sub(\"\", text)\n    cleantext = CLEANP2.sub(\"\", cleantext)\n    cleantext = CLEANP3.sub(\"\", cleantext)\n    cleantext = CLEANYTN.sub(\"\", cleantext)\n    return cleantext\ndef clean_tag(text):\n    cleantext = CLEANTAG.sub(\"\", text)\n    cleantext = CLEANSB.sub(\"\", cleantext)",
        "detail": "service.cleaner",
        "documentation": {}
    },
    {
        "label": "ModelService",
        "kind": 6,
        "importPath": "service.model_service",
        "description": "service.model_service",
        "peekOfCode": "class ModelService(BaseService):\n    \"\"\"\n    1. 자동 학습 요청 서비스\n    2. 주가 변화량 예측 요청 서비스\n    \"\"\"\n    def request_training(self):\n        news_dataset = NewsRepository(session=self.session).get_news_dataset()\n        stock_dataset = StockRepository(session=self.session).get_stock_dataset()\n        try:\n            DataController().train_news_dataset(news_dataset=news_dataset, stock_dataset=stock_dataset)",
        "detail": "service.model_service",
        "documentation": {}
    },
    {
        "label": "NewsService",
        "kind": 6,
        "importPath": "service.news_service",
        "description": "service.news_service",
        "peekOfCode": "class NewsService(BaseService):\n    def get_news_data(self, time: int, url: str) -> News:\n        response = requests.get(url, headers=self.get_header())\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        content = soup.find(\"div\", class_=\"news_view fs_type1\") if soup else \"\"\n        title = soup.find(\"h3\", class_=\"tit_view\").text if soup else \"\"\n        writer = soup.find(\"span\", class_=\"txt_info\").text if soup else \"\"\n        content = clean_text(content.text)\n        date = datetime.today().date().strftime(\"%Y%m%d\")\n        time = str(time)",
        "detail": "service.news_service",
        "documentation": {}
    },
    {
        "label": "CAPTCHA",
        "kind": 5,
        "importPath": "service.news_service",
        "description": "service.news_service",
        "peekOfCode": "CAPTCHA = \"https://captcha.search.daum.net\"\nclass NewsService(BaseService):\n    def get_news_data(self, time: int, url: str) -> News:\n        response = requests.get(url, headers=self.get_header())\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        content = soup.find(\"div\", class_=\"news_view fs_type1\") if soup else \"\"\n        title = soup.find(\"h3\", class_=\"tit_view\").text if soup else \"\"\n        writer = soup.find(\"span\", class_=\"txt_info\").text if soup else \"\"\n        content = clean_text(content.text)\n        date = datetime.today().date().strftime(\"%Y%m%d\")",
        "detail": "service.news_service",
        "documentation": {}
    },
    {
        "label": "StockService",
        "kind": 6,
        "importPath": "service.stock_service",
        "description": "service.stock_service",
        "peekOfCode": "class StockService(BaseService):\n    def get_stock_data_by_date(self, date: int, time: int):\n        return StockRepository(session=self.session).get_stock_data_by_date(date=date, time=time)\n    def save_stock_data(self, time: int, price: int):\n        price = StockRepository(session=self.session).save_stock_data(time=time, price=price)\n        news_service = NewsService(session=self.session)\n        time_now = (int(datetime.today().date().strftime(\"%Y%m%d\")) * 10000 + time) * 100\n        news_min_list = news_service.get_news_list_min(item_name=\"삼성전자\", time_now=time_now)\n        print(news_min_list)\n        for news in news_min_list:",
        "detail": "service.stock_service",
        "documentation": {}
    },
    {
        "label": "TestNewsDataController",
        "kind": 6,
        "importPath": "test.ai_model_test.data_controller_test",
        "description": "test.ai_model_test.data_controller_test",
        "peekOfCode": "class TestNewsDataController(unittest.TestCase):\n    # def test_train_train_news_dataset(self):\n    #     DataController().train_news_dataset(news_dataset=test_news_dataset, stock_dataset=test_stock_dataset)\n    def test_predict_stock_volatilities(self):\n        print(DataController().predict_stock_volatilities(text=test_text))\nif __name__ == \"__main__\":\n    unittest.main()",
        "detail": "test.ai_model_test.data_controller_test",
        "documentation": {}
    },
    {
        "label": "news_texts",
        "kind": 5,
        "importPath": "test.ai_model_test.data_controller_test",
        "description": "test.ai_model_test.data_controller_test",
        "peekOfCode": "news_texts = [\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",",
        "detail": "test.ai_model_test.data_controller_test",
        "documentation": {}
    },
    {
        "label": "news_date",
        "kind": 5,
        "importPath": "test.ai_model_test.data_controller_test",
        "description": "test.ai_model_test.data_controller_test",
        "peekOfCode": "news_date = pd.to_datetime(\n    [\n        \"2022-05-01 09:57:00\",\n        \"2022-05-03 08:01:00\",\n        \"2022-05-03 10:21:00\",\n        \"2022-05-01 09:57:00\",\n        \"2022-05-03 08:01:00\",\n        \"2022-05-03 10:21:00\",\n        \"2022-05-03 10:21:00\",\n        \"2022-05-01 09:57:00\",",
        "detail": "test.ai_model_test.data_controller_test",
        "documentation": {}
    },
    {
        "label": "test_news_dataset",
        "kind": 5,
        "importPath": "test.ai_model_test.data_controller_test",
        "description": "test.ai_model_test.data_controller_test",
        "peekOfCode": "test_news_dataset = pd.DataFrame({\"date_time\": news_date, \"content\": news_texts})\nstock_date = pd.to_datetime(\n    [\n        \"2022-05-02 09:00:00\",\n        \"2022-05-02 09:01:00\",\n        \"2022-05-02 09:05:00\",\n        \"2022-05-02 09:15:00\",\n        \"2022-05-02 10:00:00\",\n        \"2022-05-03 09:00:00\",\n        \"2022-05-03 09:01:00\",",
        "detail": "test.ai_model_test.data_controller_test",
        "documentation": {}
    },
    {
        "label": "stock_date",
        "kind": 5,
        "importPath": "test.ai_model_test.data_controller_test",
        "description": "test.ai_model_test.data_controller_test",
        "peekOfCode": "stock_date = pd.to_datetime(\n    [\n        \"2022-05-02 09:00:00\",\n        \"2022-05-02 09:01:00\",\n        \"2022-05-02 09:05:00\",\n        \"2022-05-02 09:15:00\",\n        \"2022-05-02 10:00:00\",\n        \"2022-05-03 09:00:00\",\n        \"2022-05-03 09:01:00\",\n        \"2022-05-03 09:03:00\",",
        "detail": "test.ai_model_test.data_controller_test",
        "documentation": {}
    },
    {
        "label": "stock_price",
        "kind": 5,
        "importPath": "test.ai_model_test.data_controller_test",
        "description": "test.ai_model_test.data_controller_test",
        "peekOfCode": "stock_price = [51231, 74520, 53210, 41230, 31251, 31251, 38251, 51251, 95251]\ntest_stock_dataset = pd.DataFrame({\"date_time\": stock_date, \"price\": stock_price})\ntest_text = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\"\nclass TestNewsDataController(unittest.TestCase):\n    # def test_train_train_news_dataset(self):\n    #     DataController().train_news_dataset(news_dataset=test_news_dataset, stock_dataset=test_stock_dataset)\n    def test_predict_stock_volatilities(self):\n        print(DataController().predict_stock_volatilities(text=test_text))\nif __name__ == \"__main__\":\n    unittest.main()",
        "detail": "test.ai_model_test.data_controller_test",
        "documentation": {}
    },
    {
        "label": "test_stock_dataset",
        "kind": 5,
        "importPath": "test.ai_model_test.data_controller_test",
        "description": "test.ai_model_test.data_controller_test",
        "peekOfCode": "test_stock_dataset = pd.DataFrame({\"date_time\": stock_date, \"price\": stock_price})\ntest_text = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\"\nclass TestNewsDataController(unittest.TestCase):\n    # def test_train_train_news_dataset(self):\n    #     DataController().train_news_dataset(news_dataset=test_news_dataset, stock_dataset=test_stock_dataset)\n    def test_predict_stock_volatilities(self):\n        print(DataController().predict_stock_volatilities(text=test_text))\nif __name__ == \"__main__\":\n    unittest.main()",
        "detail": "test.ai_model_test.data_controller_test",
        "documentation": {}
    },
    {
        "label": "test_text",
        "kind": 5,
        "importPath": "test.ai_model_test.data_controller_test",
        "description": "test.ai_model_test.data_controller_test",
        "peekOfCode": "test_text = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\"\nclass TestNewsDataController(unittest.TestCase):\n    # def test_train_train_news_dataset(self):\n    #     DataController().train_news_dataset(news_dataset=test_news_dataset, stock_dataset=test_stock_dataset)\n    def test_predict_stock_volatilities(self):\n        print(DataController().predict_stock_volatilities(text=test_text))\nif __name__ == \"__main__\":\n    unittest.main()",
        "detail": "test.ai_model_test.data_controller_test",
        "documentation": {}
    },
    {
        "label": "TestLdaModel",
        "kind": 6,
        "importPath": "test.ai_model_test.lda_model_test",
        "description": "test.ai_model_test.lda_model_test",
        "peekOfCode": "class TestLdaModel(unittest.TestCase):\n    def test_all_process(self):\n        ldaModel = LDAModel()\n        ldaModel.train_lda_model(dataset=dummy_dataset)\n        group_id_1, topic_distribution_1 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_1)\n        group_id_2, topic_distribution_2 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_2)\n        group_id_3, topic_distribution_3 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_3)\n        print(\"그룹 id 테스트\", group_id_1)\n        print(\"토픽 분포 테스트\", topic_distribution_1)\n        print()",
        "detail": "test.ai_model_test.lda_model_test",
        "documentation": {}
    },
    {
        "label": "dummy_dataset",
        "kind": 5,
        "importPath": "test.ai_model_test.lda_model_test",
        "description": "test.ai_model_test.lda_model_test",
        "peekOfCode": "dummy_dataset = [\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n]\ndummy_data_1 = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\"\ndummy_data_2 = \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\"\ndummy_data_3 = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\"\nclass TestLdaModel(unittest.TestCase):\n    def test_all_process(self):",
        "detail": "test.ai_model_test.lda_model_test",
        "documentation": {}
    },
    {
        "label": "dummy_data_1",
        "kind": 5,
        "importPath": "test.ai_model_test.lda_model_test",
        "description": "test.ai_model_test.lda_model_test",
        "peekOfCode": "dummy_data_1 = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\"\ndummy_data_2 = \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\"\ndummy_data_3 = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\"\nclass TestLdaModel(unittest.TestCase):\n    def test_all_process(self):\n        ldaModel = LDAModel()\n        ldaModel.train_lda_model(dataset=dummy_dataset)\n        group_id_1, topic_distribution_1 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_1)\n        group_id_2, topic_distribution_2 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_2)\n        group_id_3, topic_distribution_3 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_3)",
        "detail": "test.ai_model_test.lda_model_test",
        "documentation": {}
    },
    {
        "label": "dummy_data_2",
        "kind": 5,
        "importPath": "test.ai_model_test.lda_model_test",
        "description": "test.ai_model_test.lda_model_test",
        "peekOfCode": "dummy_data_2 = \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\"\ndummy_data_3 = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\"\nclass TestLdaModel(unittest.TestCase):\n    def test_all_process(self):\n        ldaModel = LDAModel()\n        ldaModel.train_lda_model(dataset=dummy_dataset)\n        group_id_1, topic_distribution_1 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_1)\n        group_id_2, topic_distribution_2 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_2)\n        group_id_3, topic_distribution_3 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_3)\n        print(\"그룹 id 테스트\", group_id_1)",
        "detail": "test.ai_model_test.lda_model_test",
        "documentation": {}
    },
    {
        "label": "dummy_data_3",
        "kind": 5,
        "importPath": "test.ai_model_test.lda_model_test",
        "description": "test.ai_model_test.lda_model_test",
        "peekOfCode": "dummy_data_3 = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\"\nclass TestLdaModel(unittest.TestCase):\n    def test_all_process(self):\n        ldaModel = LDAModel()\n        ldaModel.train_lda_model(dataset=dummy_dataset)\n        group_id_1, topic_distribution_1 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_1)\n        group_id_2, topic_distribution_2 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_2)\n        group_id_3, topic_distribution_3 = LDAModel().get_group_id_and_topic_distribution(text=dummy_data_3)\n        print(\"그룹 id 테스트\", group_id_1)\n        print(\"토픽 분포 테스트\", topic_distribution_1)",
        "detail": "test.ai_model_test.lda_model_test",
        "documentation": {}
    },
    {
        "label": "TestMorphsExtractor",
        "kind": 6,
        "importPath": "test.ai_model_test.morphs_extractor_test",
        "description": "test.ai_model_test.morphs_extractor_test",
        "peekOfCode": "class TestMorphsExtractor(unittest.TestCase):\n    def test_basical(self):\n        self.assertEqual(TextPreprocessor(\"[안녕]\").preprocess(), [(\"안녕\", \"NNP\")])\n    def test_stopwords(self):\n        self.assertEqual(TextPreprocessor(\"[1231241231]\").preprocess(), [])\nif __name__ == \"__main__\":\n    unittest.main()",
        "detail": "test.ai_model_test.morphs_extractor_test",
        "documentation": {}
    },
    {
        "label": "TestRegressionModel",
        "kind": 6,
        "importPath": "test.ai_model_test.regression_model_test",
        "description": "test.ai_model_test.regression_model_test",
        "peekOfCode": "class TestRegressionModel(unittest.TestCase):\n    # def test_train_lda_model(self):\n    #     lda_model = LDAModel()\n    #     num_topics = lda_model.train_lda_model(test_news_dataset)\n    #     RegressionModel(stock_dataset=test_stock_dataset, lda_model=lda_model).train_regression_model(num_topics=num_topics)\n    # def test_get_topic_distribution(self):\n    #     lda_model = LDAModel()\n    #     num_topics = lda_model.train_lda_model(test_news_dataset)\n    #     print(RegressionModel(lda_model=lda_model)._get_topic_distribution(0))\n    # def test_get_best_performance_regression_model_and_save(self):",
        "detail": "test.ai_model_test.regression_model_test",
        "documentation": {}
    },
    {
        "label": "news_texts",
        "kind": 5,
        "importPath": "test.ai_model_test.regression_model_test",
        "description": "test.ai_model_test.regression_model_test",
        "peekOfCode": "news_texts = [\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",\n    \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\",\n    \"[워싱턴=AP/뉴시스]12일(현지시간) 미국 워싱턴 백악관 루즈벨트룸에서 열린 반]\",",
        "detail": "test.ai_model_test.regression_model_test",
        "documentation": {}
    },
    {
        "label": "news_date",
        "kind": 5,
        "importPath": "test.ai_model_test.regression_model_test",
        "description": "test.ai_model_test.regression_model_test",
        "peekOfCode": "news_date = pd.to_datetime(\n    [\n        \"2022-05-01 09:57:00\",\n        \"2022-05-03 08:01:00\",\n        \"2022-05-03 10:21:00\",\n        \"2022-05-01 09:57:00\",\n        \"2022-05-03 08:01:00\",\n        \"2022-05-03 10:21:00\",\n        \"2022-05-03 10:21:00\",\n        \"2022-05-01 09:57:00\",",
        "detail": "test.ai_model_test.regression_model_test",
        "documentation": {}
    },
    {
        "label": "test_news_dataset",
        "kind": 5,
        "importPath": "test.ai_model_test.regression_model_test",
        "description": "test.ai_model_test.regression_model_test",
        "peekOfCode": "test_news_dataset = pd.DataFrame({\"date_time\": news_date, \"content\": news_texts})\nstock_date = pd.to_datetime(\n    [\n        \"2022-05-02 09:00:00\",\n        \"2022-05-02 09:01:00\",\n        \"2022-05-02 09:05:00\",\n        \"2022-05-02 09:15:00\",\n        \"2022-05-02 10:00:00\",\n        \"2022-05-03 09:00:00\",\n        \"2022-05-03 09:01:00\",",
        "detail": "test.ai_model_test.regression_model_test",
        "documentation": {}
    },
    {
        "label": "stock_date",
        "kind": 5,
        "importPath": "test.ai_model_test.regression_model_test",
        "description": "test.ai_model_test.regression_model_test",
        "peekOfCode": "stock_date = pd.to_datetime(\n    [\n        \"2022-05-02 09:00:00\",\n        \"2022-05-02 09:01:00\",\n        \"2022-05-02 09:05:00\",\n        \"2022-05-02 09:15:00\",\n        \"2022-05-02 10:00:00\",\n        \"2022-05-03 09:00:00\",\n        \"2022-05-03 09:01:00\",\n        \"2022-05-03 09:03:00\",",
        "detail": "test.ai_model_test.regression_model_test",
        "documentation": {}
    },
    {
        "label": "stock_price",
        "kind": 5,
        "importPath": "test.ai_model_test.regression_model_test",
        "description": "test.ai_model_test.regression_model_test",
        "peekOfCode": "stock_price = [51231, 74520, 53210, 41230, 31251, 31251, 38251, 51251, 95251]\ntest_stock_dataset = pd.DataFrame({\"date_time\": stock_date, \"price\": stock_price})\ntest_text = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\"\nclass TestRegressionModel(unittest.TestCase):\n    # def test_train_lda_model(self):\n    #     lda_model = LDAModel()\n    #     num_topics = lda_model.train_lda_model(test_news_dataset)\n    #     RegressionModel(stock_dataset=test_stock_dataset, lda_model=lda_model).train_regression_model(num_topics=num_topics)\n    # def test_get_topic_distribution(self):\n    #     lda_model = LDAModel()",
        "detail": "test.ai_model_test.regression_model_test",
        "documentation": {}
    },
    {
        "label": "test_stock_dataset",
        "kind": 5,
        "importPath": "test.ai_model_test.regression_model_test",
        "description": "test.ai_model_test.regression_model_test",
        "peekOfCode": "test_stock_dataset = pd.DataFrame({\"date_time\": stock_date, \"price\": stock_price})\ntest_text = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\"\nclass TestRegressionModel(unittest.TestCase):\n    # def test_train_lda_model(self):\n    #     lda_model = LDAModel()\n    #     num_topics = lda_model.train_lda_model(test_news_dataset)\n    #     RegressionModel(stock_dataset=test_stock_dataset, lda_model=lda_model).train_regression_model(num_topics=num_topics)\n    # def test_get_topic_distribution(self):\n    #     lda_model = LDAModel()\n    #     num_topics = lda_model.train_lda_model(test_news_dataset)",
        "detail": "test.ai_model_test.regression_model_test",
        "documentation": {}
    },
    {
        "label": "test_text",
        "kind": 5,
        "importPath": "test.ai_model_test.regression_model_test",
        "description": "test.ai_model_test.regression_model_test",
        "peekOfCode": "test_text = \"[ 문재인 대통령이 지난해 12월 27일 청와대에서 정부의 민관합동 청년 일자리]\"\nclass TestRegressionModel(unittest.TestCase):\n    # def test_train_lda_model(self):\n    #     lda_model = LDAModel()\n    #     num_topics = lda_model.train_lda_model(test_news_dataset)\n    #     RegressionModel(stock_dataset=test_stock_dataset, lda_model=lda_model).train_regression_model(num_topics=num_topics)\n    # def test_get_topic_distribution(self):\n    #     lda_model = LDAModel()\n    #     num_topics = lda_model.train_lda_model(test_news_dataset)\n    #     print(RegressionModel(lda_model=lda_model)._get_topic_distribution(0))",
        "detail": "test.ai_model_test.regression_model_test",
        "documentation": {}
    },
    {
        "label": "AddExcel",
        "kind": 6,
        "importPath": "test.add_excel_data",
        "description": "test.add_excel_data",
        "peekOfCode": "class AddExcel:\n    def add_excel(self):\n        create_db()\n        file_path = \"/home/tako4/capstone/backend/Backend/data/daum_samsung_20220601000000_202206292353.csv\"\n        session = get_session()\n        df = pd.read_csv(file_path)\n        num_rows = len(df)\n        # print(df)\n        for i in range(num_rows):\n            content = df.iloc[i][\"content\"]",
        "detail": "test.add_excel_data",
        "documentation": {}
    },
    {
        "label": "excel",
        "kind": 5,
        "importPath": "test.add_excel_data",
        "description": "test.add_excel_data",
        "peekOfCode": "excel = AddExcel()\nexcel.add_excel()",
        "detail": "test.add_excel_data",
        "documentation": {}
    },
    {
        "label": "AddStock",
        "kind": 6,
        "importPath": "test.add_stock",
        "description": "test.add_stock",
        "peekOfCode": "class AddStock:\n    def add_excel(self):\n        create_db()\n        file_path = \"/home/tako4/capstone/backend/Backend/data/samsung_minute_chart_data_20220419_20240429.csv\"\n        session = get_session()\n        df = pd.read_csv(file_path)\n        num_rows = len(df)\n        # print(df)\n        for i in range(num_rows):\n            date = df.iloc[i][\"Date\"]",
        "detail": "test.add_stock",
        "documentation": {}
    },
    {
        "label": "excel",
        "kind": 5,
        "importPath": "test.add_stock",
        "description": "test.add_stock",
        "peekOfCode": "excel = AddStock()\nexcel.add_excel()",
        "detail": "test.add_stock",
        "documentation": {}
    },
    {
        "label": "CpEvent",
        "kind": 6,
        "importPath": "test.stock_current_test",
        "description": "test.stock_current_test",
        "peekOfCode": "class CpEvent:\n    def set_params(self, client):\n        self.client = client\n    def OnReceived(self):\n        code = self.client.GetHeaderValue(0)  # 초\n        name = self.client.GetHeaderValue(1)  # 초\n        timess = self.client.GetHeaderValue(18)  # 초\n        exFlag = self.client.GetHeaderValue(19)  # 예상체결 플래그\n        cprice = self.client.GetHeaderValue(13)  # 현재가\n        diff = self.client.GetHeaderValue(2)  # 대비",
        "detail": "test.stock_current_test",
        "documentation": {}
    },
    {
        "label": "CpStockCur",
        "kind": 6,
        "importPath": "test.stock_current_test",
        "description": "test.stock_current_test",
        "peekOfCode": "class CpStockCur:\n    def Subscribe(self, code):\n        self.objStockCur = win32com.client.Dispatch(\"DsCbo1.StockCur\")\n        handler = win32com.client.WithEvents(self.objStockCur, CpEvent)\n        self.objStockCur.SetInputValue(0, code)\n        handler.set_params(self.objStockCur)\n        self.objStockCur.Subscribe()\n    def Unsubscribe(self):\n        self.objStockCur.Unsubscribe()\n# Cp6033 : 주식 잔고 조회",
        "detail": "test.stock_current_test",
        "documentation": {}
    },
    {
        "label": "Cp6033",
        "kind": 6,
        "importPath": "test.stock_current_test",
        "description": "test.stock_current_test",
        "peekOfCode": "class Cp6033:\n    def __init__(self):\n        # 통신 OBJECT 기본 세팅\n        self.objTrade = win32com.client.Dispatch(\"CpTrade.CpTdUtil\")\n        initCheck = self.objTrade.TradeInit(0)\n        if initCheck != 0:\n            print(\"주문 초기화 실패\")\n            return\n        #\n        acc = self.objTrade.AccountNumber[0]  # 계좌번호",
        "detail": "test.stock_current_test",
        "documentation": {}
    },
    {
        "label": "CpMarketEye",
        "kind": 6,
        "importPath": "test.stock_current_test",
        "description": "test.stock_current_test",
        "peekOfCode": "class CpMarketEye:\n    def Request(self, codes, rqField):\n        # 연결 여부 체크\n        objCpCybos = win32com.client.Dispatch(\"CpUtil.CpCybos\")\n        bConnect = objCpCybos.IsConnect\n        if bConnect == 0:\n            print(\"PLUS가 정상적으로 연결되지 않음. \")\n            return False\n        # 관심종목 객체 구하기\n        objRq = win32com.client.Dispatch(\"CpSysDib.MarketEye\")",
        "detail": "test.stock_current_test",
        "documentation": {}
    },
    {
        "label": "MyWindow",
        "kind": 6,
        "importPath": "test.stock_current_test",
        "description": "test.stock_current_test",
        "peekOfCode": "class MyWindow(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"PLUS API TEST\")\n        self.setGeometry(300, 300, 300, 180)\n        self.isSB = False\n        self.objCur = []\n        btnStart = QPushButton(\"요청 시작\", self)\n        btnStart.move(20, 20)\n        btnStart.clicked.connect(self.btnStart_clicked)",
        "detail": "test.stock_current_test",
        "documentation": {}
    },
    {
        "label": "StockRepository",
        "kind": 6,
        "importPath": "test.stock_test",
        "description": "test.stock_test",
        "peekOfCode": "class StockRepository:\n    def __init__(self) -> None:\n        self.inst_stock_chart = win32com.client.Dispatch(\"CpSysDib.StockChart\")\n    def get_stock_data_by_date(self, item_code: str, news_datetime: str):\n        today = datetime.datetime.today().strftime(\"%Y%m%d\")\n        today = \"20240212\"\n        self.inst_stock_chart.SetInputValue(0, item_code)  # 종목코드\n        self.inst_stock_chart.SetInputValue(1, ord(\"2\"))  # 개수로 받기\n        # self.inst_stock_chart.SetInputValue(2, today)\n        self.inst_stock_chart.SetInputValue(4, 1)  # 최근 1개",
        "detail": "test.stock_test",
        "documentation": {}
    },
    {
        "label": "SAMSUNG_CODE",
        "kind": 5,
        "importPath": "test.stock_test",
        "description": "test.stock_test",
        "peekOfCode": "SAMSUNG_CODE = \"A005930\"\nclass StockRepository:\n    def __init__(self) -> None:\n        self.inst_stock_chart = win32com.client.Dispatch(\"CpSysDib.StockChart\")\n    def get_stock_data_by_date(self, item_code: str, news_datetime: str):\n        today = datetime.datetime.today().strftime(\"%Y%m%d\")\n        today = \"20240212\"\n        self.inst_stock_chart.SetInputValue(0, item_code)  # 종목코드\n        self.inst_stock_chart.SetInputValue(1, ord(\"2\"))  # 개수로 받기\n        # self.inst_stock_chart.SetInputValue(2, today)",
        "detail": "test.stock_test",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 6,
        "importPath": "test.test_crud_df",
        "description": "test.test_crud_df",
        "peekOfCode": "class test(unittest.TestCase):\n    def test_stock(self):\n        session = get_session()\n        yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n        stmt = select(News).where(News.date_time < yesterday)\n        all_news = session.execute(stmt)\n        all_news = all_news.scalars().all()\n        df = pd.DataFrame([item.__dict__ for item in all_news])\n        if \"_sa_instance_state\" in df.columns:\n            df = df.drop(columns=[\"_sa_instance_state\"])",
        "detail": "test.test_crud_df",
        "documentation": {}
    },
    {
        "label": "session",
        "kind": 5,
        "importPath": "test.test_crud_df",
        "description": "test.test_crud_df",
        "peekOfCode": "session = get_session()\n# yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n# stmt = select(News).where(News.date_time < yesterday)\n# all_news = session.execute(stmt)\n# all_news = all_news.scalars().all()\n# df = pd.DataFrame([item.__dict__ for item in all_news])\n# if \"_sa_instance_state\" in df.columns:\n#     df = df.drop(columns=[\"_sa_instance_state\"])\n# if \"id\" in df.columns:\n#     df = df.drop(columns=[\"id\"])",
        "detail": "test.test_crud_df",
        "documentation": {}
    },
    {
        "label": "news_dataset",
        "kind": 5,
        "importPath": "test.test_crud_df",
        "description": "test.test_crud_df",
        "peekOfCode": "news_dataset = NewsRepository(session=session).get_news_dataset()\nstock_dataset = StockRepository(session=session).get_stock_dataset()\ntry:\n    DataController().train_news_dataset(news_dataset=news_dataset, stock_dataset=stock_dataset)\n    print(\"done\")\nexcept Exception as ex:\n    print(\"무슨 에러지?\", ex)",
        "detail": "test.test_crud_df",
        "documentation": {}
    },
    {
        "label": "stock_dataset",
        "kind": 5,
        "importPath": "test.test_crud_df",
        "description": "test.test_crud_df",
        "peekOfCode": "stock_dataset = StockRepository(session=session).get_stock_dataset()\ntry:\n    DataController().train_news_dataset(news_dataset=news_dataset, stock_dataset=stock_dataset)\n    print(\"done\")\nexcept Exception as ex:\n    print(\"무슨 에러지?\", ex)",
        "detail": "test.test_crud_df",
        "documentation": {}
    },
    {
        "label": "time_now",
        "kind": 5,
        "importPath": "test.test_time",
        "description": "test.test_time",
        "peekOfCode": "time_now = datetime.now().time()\ntime_now = time_now.strftime(\"%H%M\")\nprint(time_now)",
        "detail": "test.test_time",
        "documentation": {}
    },
    {
        "label": "time_now",
        "kind": 5,
        "importPath": "test.test_time",
        "description": "test.test_time",
        "peekOfCode": "time_now = time_now.strftime(\"%H%M\")\nprint(time_now)",
        "detail": "test.test_time",
        "documentation": {}
    },
    {
        "label": "get_header",
        "kind": 2,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "def get_header():\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.183 Safari/537.36 Vivaldi/1.96.1147.47\",\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n    }\n    return headers\npage = 1\nstock = \"삼성전자\"\nbase_url = f\"https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q={stock}&p={page}\"\nis_next_page = True",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "page",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "page = 1\nstock = \"삼성전자\"\nbase_url = f\"https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q={stock}&p={page}\"\nis_next_page = True\nresponse = requests.get(base_url, headers=get_header())\nlist_soup = BeautifulSoup(response.text, \"html.parser\")\n# 리스트 전체 가져오기\n# 리스트에서 필요한거 뽑아서 객체 생성해저 집어넣기\n# 리스트 반환하기\nnews_datas = list_soup.find(\"ul\", class_=\"c-list-basic\")",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "stock",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "stock = \"삼성전자\"\nbase_url = f\"https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q={stock}&p={page}\"\nis_next_page = True\nresponse = requests.get(base_url, headers=get_header())\nlist_soup = BeautifulSoup(response.text, \"html.parser\")\n# 리스트 전체 가져오기\n# 리스트에서 필요한거 뽑아서 객체 생성해저 집어넣기\n# 리스트 반환하기\nnews_datas = list_soup.find(\"ul\", class_=\"c-list-basic\")\nbs_list = news_datas.find_all(\"li\") if news_datas else []",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "base_url",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "base_url = f\"https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q={stock}&p={page}\"\nis_next_page = True\nresponse = requests.get(base_url, headers=get_header())\nlist_soup = BeautifulSoup(response.text, \"html.parser\")\n# 리스트 전체 가져오기\n# 리스트에서 필요한거 뽑아서 객체 생성해저 집어넣기\n# 리스트 반환하기\nnews_datas = list_soup.find(\"ul\", class_=\"c-list-basic\")\nbs_list = news_datas.find_all(\"li\") if news_datas else []\nnews_list = []",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "is_next_page",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "is_next_page = True\nresponse = requests.get(base_url, headers=get_header())\nlist_soup = BeautifulSoup(response.text, \"html.parser\")\n# 리스트 전체 가져오기\n# 리스트에서 필요한거 뽑아서 객체 생성해저 집어넣기\n# 리스트 반환하기\nnews_datas = list_soup.find(\"ul\", class_=\"c-list-basic\")\nbs_list = news_datas.find_all(\"li\") if news_datas else []\nnews_list = []\nfor item in bs_list:",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "response = requests.get(base_url, headers=get_header())\nlist_soup = BeautifulSoup(response.text, \"html.parser\")\n# 리스트 전체 가져오기\n# 리스트에서 필요한거 뽑아서 객체 생성해저 집어넣기\n# 리스트 반환하기\nnews_datas = list_soup.find(\"ul\", class_=\"c-list-basic\")\nbs_list = news_datas.find_all(\"li\") if news_datas else []\nnews_list = []\nfor item in bs_list:\n    print(item)",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "list_soup",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "list_soup = BeautifulSoup(response.text, \"html.parser\")\n# 리스트 전체 가져오기\n# 리스트에서 필요한거 뽑아서 객체 생성해저 집어넣기\n# 리스트 반환하기\nnews_datas = list_soup.find(\"ul\", class_=\"c-list-basic\")\nbs_list = news_datas.find_all(\"li\") if news_datas else []\nnews_list = []\nfor item in bs_list:\n    print(item)\n    news_list.append(item)",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "news_datas",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "news_datas = list_soup.find(\"ul\", class_=\"c-list-basic\")\nbs_list = news_datas.find_all(\"li\") if news_datas else []\nnews_list = []\nfor item in bs_list:\n    print(item)\n    news_list.append(item)\n# print(news_list)",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "bs_list",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "bs_list = news_datas.find_all(\"li\") if news_datas else []\nnews_list = []\nfor item in bs_list:\n    print(item)\n    news_list.append(item)\n# print(news_list)",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "news_list",
        "kind": 5,
        "importPath": "test.testing",
        "description": "test.testing",
        "peekOfCode": "news_list = []\nfor item in bs_list:\n    print(item)\n    news_list.append(item)\n# print(news_list)",
        "detail": "test.testing",
        "documentation": {}
    },
    {
        "label": "Base",
        "kind": 6,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "class Base(DeclarativeBase):\n    def __repr__(self) -> str:\n        return str({c.key: getattr(self, c.key) for c in inspect(self).mapper.column_attrs})\ndef get_engine():\n    engine = create_engine(DATABASE_URL)\n    return engine\ndef get_session():\n    engine = get_engine()\n    session = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n    return session()",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_engine",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def get_engine():\n    engine = create_engine(DATABASE_URL)\n    return engine\ndef get_session():\n    engine = get_engine()\n    session = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n    return session()\ndef create_db():\n    engine = get_engine()\n    Base.metadata.create_all(bind=engine)",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_session",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def get_session():\n    engine = get_engine()\n    session = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n    return session()\ndef create_db():\n    engine = get_engine()\n    Base.metadata.create_all(bind=engine)",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "create_db",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def create_db():\n    engine = get_engine()\n    Base.metadata.create_all(bind=engine)",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "user",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "user = \"cap\"\npassword = \"dusrntlf512\"\nsever = \"localhost:5432\"\ndatabase = \"capstone\"\nDATABASE_URL = f\"postgresql+psycopg2://{user}:{password}@{sever}/{database}\"\nclass Base(DeclarativeBase):\n    def __repr__(self) -> str:\n        return str({c.key: getattr(self, c.key) for c in inspect(self).mapper.column_attrs})\ndef get_engine():\n    engine = create_engine(DATABASE_URL)",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "password",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "password = \"dusrntlf512\"\nsever = \"localhost:5432\"\ndatabase = \"capstone\"\nDATABASE_URL = f\"postgresql+psycopg2://{user}:{password}@{sever}/{database}\"\nclass Base(DeclarativeBase):\n    def __repr__(self) -> str:\n        return str({c.key: getattr(self, c.key) for c in inspect(self).mapper.column_attrs})\ndef get_engine():\n    engine = create_engine(DATABASE_URL)\n    return engine",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "sever",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "sever = \"localhost:5432\"\ndatabase = \"capstone\"\nDATABASE_URL = f\"postgresql+psycopg2://{user}:{password}@{sever}/{database}\"\nclass Base(DeclarativeBase):\n    def __repr__(self) -> str:\n        return str({c.key: getattr(self, c.key) for c in inspect(self).mapper.column_attrs})\ndef get_engine():\n    engine = create_engine(DATABASE_URL)\n    return engine\ndef get_session():",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "database",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "database = \"capstone\"\nDATABASE_URL = f\"postgresql+psycopg2://{user}:{password}@{sever}/{database}\"\nclass Base(DeclarativeBase):\n    def __repr__(self) -> str:\n        return str({c.key: getattr(self, c.key) for c in inspect(self).mapper.column_attrs})\ndef get_engine():\n    engine = create_engine(DATABASE_URL)\n    return engine\ndef get_session():\n    engine = get_engine()",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "DATABASE_URL",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "DATABASE_URL = f\"postgresql+psycopg2://{user}:{password}@{sever}/{database}\"\nclass Base(DeclarativeBase):\n    def __repr__(self) -> str:\n        return str({c.key: getattr(self, c.key) for c in inspect(self).mapper.column_attrs})\ndef get_engine():\n    engine = create_engine(DATABASE_URL)\n    return engine\ndef get_session():\n    engine = get_engine()\n    session = sessionmaker(autocommit=False, autoflush=False, bind=engine)",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app = FastAPI()\napp.include_router(stock_router, prefix=\"/stock\")\napp.include_router(news_router, prefix=\"/news\")\napp.include_router(model_router, prefix=\"/model\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],",
        "detail": "main",
        "documentation": {}
    }
]